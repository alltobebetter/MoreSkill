# 深度学习基础

## 什么是深度学习

深度学习是机器学习的一个分支，使用多层神经网络来学习数据的复杂模式。

比喻：传统机器学习像手动调收音机（你选特征），深度学习像智能收音机（它自己找到最佳频率）。

## 神经网络

```
输入层 → 隐藏层1 → 隐藏层2 → ... → 输出层

每一层由多个"神经元"组成
每个神经元：接收输入 → 加权求和 → 激活函数 → 输出
```

"深度"就是指隐藏层的数量。层数越多，能学习的模式越复杂。

## 核心概念

| 概念 | 说明 |
|------|------|
| 权重（Weight） | 连接的强度，训练过程中不断调整 |
| 偏置（Bias） | 调整输出的阈值 |
| 激活函数 | 引入非线性（ReLU、Sigmoid） |
| 损失函数 | 衡量预测和真实值的差距 |
| 反向传播 | 根据损失调整权重的算法 |
| 梯度下降 | 优化权重的方法 |
| Epoch | 完整遍历一次训练数据 |
| Batch Size | 每次训练用多少样本 |
| 学习率 | 每次调整权重的步长 |

## 常见网络类型

| 类型 | 缩写 | 适用场景 |
|------|------|---------|
| 卷积神经网络 | CNN | 图像识别、计算机视觉 |
| 循环神经网络 | RNN | 序列数据、时间序列 |
| Transformer | - | NLP、大语言模型 |
| GAN | - | 图像生成 |
| 自编码器 | AE | 降维、异常检测 |

## PyTorch 简单示例

```python
import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(784, 128),   # 输入层 → 隐藏层
            nn.ReLU(),
            nn.Linear(128, 10)     # 隐藏层 → 输出层
        )

    def forward(self, x):
        return self.layers(x)

model = SimpleNet()
```

## GPU 加速

深度学习需要大量矩阵运算，GPU 比 CPU 快几十到几百倍。

| 框架 | 特点 |
|------|------|
| PyTorch | 灵活，学术界主流 |
| TensorFlow | Google 出品，生产部署成熟 |
| JAX | Google 新框架，函数式风格 |

## 学习路径

1. 理解基本概念（神经元、反向传播）
2. 用 PyTorch 跑通一个简单模型
3. 学习 CNN（图像）或 Transformer（文本）
4. 了解预训练模型和迁移学习
5. 实际项目练手

> 深度学习的门槛在降低，但理解原理仍然重要。不要只会调 API，要知道为什么这样做。
